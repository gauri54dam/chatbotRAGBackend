{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fba2a913-bdbd-469f-8250-c4b3988ec2a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Install modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f443b5e3-02da-4f50-a7f9-6cf80e9713b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install mlflow langchain databricks-vectorsearch databricks-sdk mlflow[databricks] dotenv langchain_community langchain-core langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90e237ce-8b28-40ca-9743-1f2e4748a2b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f64d5c6-9d77-42dd-912d-b049a0fc5226",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c9416f3-4f20-45f7-99c9-8983ab25e535",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1370e87d-f552-4061-bc0e-f5ed1cf63781",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cf689973-e1ca-4b5b-977b-5d7f30334e2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ca83eff-6c5f-48b3-8504-563458ca6f92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4e342aa-dc6a-4199-9888-4a110cf17c6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import IPython\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "630379a8-815a-48c7-a696-499101a017de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Langraph components ---------\n",
    "from typing import Annotated, List, Dict, Any\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.messages import AnyMessage\n",
    "from langgraph.graph import StateGraph\n",
    "from IPython.display import Image, display\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "memory: MemorySaver = MemorySaver()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Databricks models and embeddings -------------\n",
    "from langchain_community.chat_models import ChatDatabricks\n",
    "from langchain_community.embeddings import DatabricksEmbeddings\n",
    "\n",
    "# vector search ----------\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "from langchain_community.vectorstores import DatabricksVectorSearch\n",
    "\n",
    "# MLFLow -------------\n",
    "from mlflow.models import infer_signature\n",
    "import mlflow\n",
    "import langchain\n",
    "import langchain_core\n",
    "import langchain_community\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langgraph.graph import START, END \n",
    "\n",
    "\n",
    "# UI\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec4b9fab-9909-4b7d-b474-8aeb375f8ca5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Set env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e9440b6-0ae7-4425-8d29-6bbea7ad575c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "host = \"https://\" + spark.conf.get(\"spark.databricks.workspaceUrl\")\n",
    "os.environ[\"DATABRICKS_HOST\"] = host\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92cddcf2-3a62-400c-8752-87b4eb0a081f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f2d4623-c155-4650-834f-82ba6c0b2ed3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Databricks model and embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f17276c8-5fab-4631-be4a-612339cbf177",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "chat_model = ChatDatabricks(endpoint=\"databricks-meta-llama-3-3-70b-instruct\", max_tokens = 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aef0c457-2e80-43ef-bf50-5606f1a3fefd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "embedding_model = DatabricksEmbeddings(endpoint=\"databricks-bge-large-en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9bb2fdf6-db79-4270-94a4-2be9cadda506",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Create Vector DB retriever function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9f5caae-9e84-42b5-9b0c-8d5ced53d235",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "index_name=\"analytics.models.docs_idxs\"\n",
    "VECTOR_SEARCH_ENDPOINT_NAME=\"document_vector_endpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb590090-e319-4b63-9fb4-886f78e98290",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# --- vectorestore retriver \n",
    "def get_retriever(persist_dir: str = None):\n",
    "    os.environ[\"DATABRICKS_HOST\"] = host\n",
    "    #Get the vector search index\n",
    "    vsc = VectorSearchClient(workspace_url=host, personal_access_token=os.environ[\"DATABRICKS_TOKEN\"])\n",
    "    vs_index = vsc.get_index(\n",
    "        endpoint_name=VECTOR_SEARCH_ENDPOINT_NAME,\n",
    "        index_name=index_name\n",
    "    )\n",
    "\n",
    "    # Create the retriever\n",
    "    vectorstore = DatabricksVectorSearch(\n",
    "        vs_index, text_column=\"text\", embedding=embedding_model\n",
    "    )\n",
    "    return vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d541743-4b2d-4f9a-98e0-1550ce6b6f7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- web search tool\n",
    "def web_search_tool(query: str) -> str:\n",
    "    headers = {\n",
    "    \"X-API-KEY\": os.environ[\"SERPAPI_KEY\"],\n",
    "    \"Content-Type\": \"application/json\"\n",
    "        }\n",
    "\n",
    "    params = {\n",
    "    \"q\": query\n",
    "    }\n",
    "\n",
    "    res = requests.post(\"https://google.serper.dev/search\", json=params, headers=headers)\n",
    "    res_json = res.json()\n",
    "    results = res_json.get(\"organic\", [])\n",
    "  \n",
    "    if results:\n",
    "        return results[0].get(\"snippet\", \"\")\n",
    "    return \"No useful web data found.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ded3a21a-67da-4944-84d2-dea272a33ba2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Compile Langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97b70237-0c00-44aa-b383-f0ad3db2450e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "retriever = get_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d7d5d04-a4e3-469d-80d1-00b2f8e37648",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- Langgraph State Definition\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[List[AnyMessage], add_messages]\n",
    "    retrieved_context: str\n",
    "    initial_chatbot_response: str\n",
    "\n",
    "# --- Langgraph Graph Builder\n",
    "graph_builder = StateGraph(State)\n",
    "retriever = get_retriever()\n",
    "\n",
    "\n",
    "def retrieval_node(state: State):\n",
    "\n",
    "    print(\"Executing conversational retrieval...\")\n",
    "    conversation_for_retrieval = \"\\n\".join([msg.content for msg in state['messages']])\n",
    "    if not conversation_for_retrieval.strip():\n",
    "        return {\"retrieved_context\": \"No query provided.\"}\n",
    "    docs = retriever.get_relevant_documents(conversation_for_retrieval)\n",
    "    if not docs:\n",
    "        context = \"No relevant HR policy document was found for the query.\"\n",
    "    else:\n",
    "        context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    return {\"retrieved_context\": context}\n",
    "\n",
    "\n",
    "def chatbot_node(state: State):\n",
    "\n",
    "    prompt = f\"\"\"You are an HR Assistant chatbot designed to provide accurate and up-to-date answers to employees HR-related queries. Your responses should be clear, concise, and directly based on the company's official policies. If the question is not related to one of these topics, kindly decline to answer. If you don't know the answer, say that you don't know; don't try to make up an HR policy answer. Keep the answer as concise as possible. Provide all answers only in English. Use the following pieces of context to answer the HR Policy question:\n",
    "    {state['retrieved_context']}\"\"\"\n",
    "\n",
    "    llm_messages = [SystemMessage(content=prompt)]\n",
    "\n",
    "    llm_messages.extend(state['messages']) # Adds all messages from the current conversation\n",
    "\n",
    "    response = chat_model.invoke(llm_messages)\n",
    "    return {\"messages\": [response],\"initial_chatbot_response\": response.content}\n",
    "    \n",
    " \n",
    "\n",
    "def enhancer_node(state: State):\n",
    " \n",
    "    user_query = [msg for msg in state[\"messages\"] if isinstance(msg, HumanMessage)][-1].content\n",
    "    initial_answer = state[\"initial_chatbot_response\"]\n",
    "    retrieved_hr_context = state[\"retrieved_context\"]\n",
    "    current_date_str = datetime.date.today().strftime(\"%B %d, %Y\")\n",
    "\n",
    "    web_context = web_search_tool(user_query)\n",
    "    print(f\"Web search conducted. Results: {web_context[:100]}...\")\n",
    "\n",
    "    synthesis_prompt = f\"\"\"\n",
    "    You are an HR Assistant chatbot providing comprehensive and accurate answers.\n",
    "    You have already generated an initial response based on internal HR policies.\n",
    "    Now, you have additional external information from a web search.\n",
    "\n",
    "    Your task is to **synthesize** all available information to provide the best possible answer to the user.\n",
    "    **Do NOT simply copy-paste directly from the web search results.**\n",
    "    Integrate the web search results with the initial HR policy response, making sure to:\n",
    "    - Add any missing factual details (like specific dates or external holiday names if relevant and not in HR policy).\n",
    "    - Clarify or correct information if the web search provides more accurate external facts.\n",
    "    - Prioritize information from the **internal HR policy** for HR-related questions. Use web search primarily for external facts (e.g., current date-specific holidays, general knowledge).\n",
    "    - If the web search provides no new relevant information, you can mostly stick to the initial answer, perhaps rephrasing slightly for clarity.\n",
    "    - If the web search contradicts internal HR policy on a *policy* matter, state that the company policy is the primary source.\n",
    "    - Maintain a neutral, respectful, and concise tone.\n",
    "    - Provide all answers only in English.\n",
    "\n",
    "    --- User Query ---\n",
    "    {user_query}\n",
    "\n",
    "    --- Initial HR Policy Response ---\n",
    "    {initial_answer}\n",
    "\n",
    "    --- Internal HR Policy Context (from retrieval) ---\n",
    "    {retrieved_hr_context}\n",
    "\n",
    "    --- External Web Search Results (if available) ---\n",
    "    {web_context}\n",
    "\n",
    "    --- Current Date ---\n",
    "    {current_date_str}\n",
    "\n",
    "    Based on the above, provide the final, synthesized answer:\n",
    "    \"\"\"\n",
    "    llm_messages_for_llm = [SystemMessage(content=synthesis_prompt)]\n",
    "    \n",
    "\n",
    "    last_human_message = next((msg for msg in reversed(state['messages']) if isinstance(msg, HumanMessage)), None)\n",
    "    if last_human_message:\n",
    "        llm_messages_for_llm.append(last_human_message)\n",
    "    else:\n",
    "        print(\"Warning: No human message found in state for enhanced_response_node.\")\n",
    "        \n",
    "    final_response = chat_model.invoke(llm_messages_for_llm)\n",
    "    \n",
    "    # LangGraph's 'add_messages' will correctly append this new AIMessage to state['messages']\n",
    "    # This also effectively replaces the 'initial_chatbot_response' in the conversation history\n",
    "    return {\"messages\": [final_response]}\n",
    "\n",
    "\n",
    "# --- The Router Node ---\n",
    "def should_enhance(state: State) -> str:\n",
    "    \"\"\"\n",
    "    Uses an LLM to decide if the user's query requires a web search for external facts.\n",
    "    \"\"\"\n",
    "    \n",
    "    user_query = [msg.content for msg in state[\"messages\"] if isinstance(msg, HumanMessage)][-1]\n",
    "    \n",
    "    # Use an LLM to make the routing decision\n",
    "    routing_prompt = f\"\"\"You are a routing agent. Your goal is to decide if a user's query requires a web search for external, factual information (like specific dates, holidays, states, current events) or if it can be answered solely by an internal HR policy document.\n",
    "\n",
    "    User Query: \"{user_query}\"\n",
    "\n",
    "    Does this query likely require a web search for up-to-date, external facts? Answer only with the single word 'yes' or 'no'.\n",
    "    \"\"\"\n",
    "    response = chat_model.invoke(routing_prompt)\n",
    "    decision = response.content.strip().lower()\n",
    "    print(f\"--- [Router] LLM decision: '{decision}' ---\")\n",
    "\n",
    "    if 'yes' in decision:\n",
    "        print(\"--- [Router] Path chosen: enhance ---\")\n",
    "        return \"enhance\"\n",
    "    else:\n",
    "        print(\"--- [Router] Path chosen: end ---\")\n",
    "        return \"end\"\n",
    "\n",
    "# --- Add Nodes to the Graph ---\n",
    "graph_builder.add_node(\"retrieval_node\", retrieval_node)\n",
    "graph_builder.add_node(\"chatbot_node\", chatbot_node)\n",
    "graph_builder.add_node(\"enhancer_node\", enhancer_node)\n",
    "\n",
    "# --- Define Graph Edges with Conditional Logic ---\n",
    "graph_builder.add_edge(START, \"retrieval_node\")\n",
    "graph_builder.add_edge(\"retrieval_node\", \"chatbot_node\")\n",
    "\n",
    "# After the chatbot node, we call the router to decide the next step\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot_node\",\n",
    "    should_enhance,\n",
    "    {\n",
    "        # If the router returns \"enhance\", we go to the enhancer_node.\n",
    "        \"enhance\": \"enhancer_node\",\n",
    "        # If the router returns \"end\", we finish the graph execution.\n",
    "        \"end\": END\n",
    "    }\n",
    ")\n",
    "\n",
    "graph_builder.add_edge(\"enhancer_node\", END)\n",
    "\n",
    "# --- Compile the Graph ---\n",
    "graph = graph_builder.compile(checkpointer=memory)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f69b19d-57a1-4125-afd1-b2bd6018e308",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "643da922-b2e1-48f7-b6c5-19e6aebbb9a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65f575cf-c518-4ab5-9e74-4ad67426553a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "graph.nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "187b99d5-70ca-4a90-bdfb-1cf1a82c2db9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from langchain_core.runnables import Runnable\n",
    "import uuid\n",
    "from typing import Any, Dict, Optional\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "\n",
    "class LanggraphRunnable(Runnable):\n",
    "\n",
    "    def invoke(self, input_text: str, config: Optional[Dict] = None) -> str:\n",
    "        run_config = config or {}\n",
    "        \n",
    "        # Define the input payload for the graph\n",
    "        input_payload = {'messages': [HumanMessage(content=input_text)]}\n",
    "\n",
    "        final_state = graph.invoke(input_payload, config=run_config)\n",
    "        \n",
    "        # Safely extract the last message from the final state dictionary\n",
    "        if final_state and \"messages\" in final_state:\n",
    "            if final_state[\"messages\"]:\n",
    "                last_message = final_state[\"messages\"][-1]\n",
    "                if isinstance(last_message, AIMessage):\n",
    "                    return last_message.content\n",
    "\n",
    "        # This will be returned only if something goes wrong\n",
    "        return \"Error: Could not extract a valid response from the chatbot.\"\n",
    "\n",
    "langraph_runnable = LanggraphRunnable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cdbfd27-2454-4a49-a4de-14b1de46dc79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "question = \"how do i do expenses when I travel?\"\n",
    "current_conversation_id = str(uuid.uuid4())\n",
    "\n",
    "run_config = {\"configurable\": {\"thread_id\": current_conversation_id}}\n",
    "answer = langraph_runnable.invoke(question, config=run_config)\n",
    "\n",
    "\n",
    "if answer:\n",
    "    print(\"Chatbot Response:\", answer)\n",
    "else:\n",
    "    print(\"No response from the chatbot.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d94b1b42-46c9-42cd-9c1c-5c27bf3d1580",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "question = \"how do I do expenses when I travel?\"\n",
    "answer = langraph_runnable.invoke(question,  config=run_config)\n",
    "\n",
    "if answer:\n",
    "    print(\"Chatbot Response:\", answer)\n",
    "else:\n",
    "    print(\"No response from the chatbot.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b836d23-42e8-44f4-abd0-2b4de705ab81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "question = \"How do I do expenses when I travel?\"\n",
    "answer = langraph_runnable.invoke(question, config=run_config)\n",
    "\n",
    "if answer:\n",
    "    print(\"Chatbot Response:\", answer)\n",
    "else:\n",
    "    print(\"No response from the chatbot.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22c8d5a4-9fa7-45c4-89b2-3bd759e4d93d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "question = \"What are the key takeaways of harrassment policy?\"\n",
    "answer = langraph_runnable.invoke(question, config=run_config)\n",
    "\n",
    "if answer:\n",
    "    print(\"Chatbot Response:\", answer)\n",
    "else:\n",
    "    print(\"No response from the chatbot.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6979c02c-5c38-45f0-81f2-196ac3aa65af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Simple Gradio UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fd1697a-d288-47c9-a2a1-acf5620c9bb1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def predict(question):\n",
    "    answer = langraph_runnable.invoke(question)\n",
    "    return answer if answer else \"No response from the chatbot.\"\n",
    "\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=predict,\n",
    "    inputs=gr.Textbox(label=\"Ask your HR question\"),\n",
    "    outputs=gr.Textbox(label=\"HR Assistant Response\"),\n",
    "    title=\"HR Assistant Chatbot\",\n",
    "    description=\"Ask any HR-related question and I will try my best to answer based on company policies.\"\n",
    ")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "79463f19-1f68-4c77-9159-61382f75fc5c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Streamlit UI with Conversation History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d17437d3-510f-4875-a28b-5e94ca18d558",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import streamlit as st\n",
    "import requests\n",
    "\n",
    "## langgraph\n",
    "from langchain_core.messages import HumanMessage, AIMessage, AnyMessage\n",
    "from langchain_core.runnables import Runnable\n",
    "\n",
    "## mlflow\n",
    "import mlflow\n",
    "\n",
    "\n",
    "loaded_model = graph\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a905577-1196-4ac2-bb54-eee2db807713",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class LanggraphRunnable(Runnable):\n",
    "    def __init__(self, user_name: str = None):\n",
    "        self.user_name = user_name\n",
    "\n",
    "    def invoke(self, input: str, history: list[dict] = None):\n",
    "        message_objects = []\n",
    "\n",
    "        if history:\n",
    "            for msg in history:\n",
    "                if msg[\"role\"] == \"user\":\n",
    "                    content = msg[\"content\"]\n",
    "                    if self.user_name:\n",
    "                        content = f\"Please remember, my name is {self.user_name}. \" + content\n",
    "                    message_objects.append(HumanMessage(content=content))\n",
    "                elif msg[\"role\"] == \"assistant\":\n",
    "                    message_objects.append(AIMessage(content=msg[\"content\"]))\n",
    "\n",
    "        # Inject instruction into current input\n",
    "        if self.user_name:\n",
    "            input = f\"Please remember, my name is {self.user_name}. \" + input\n",
    "\n",
    "        message_objects.append(HumanMessage(content=input))\n",
    "\n",
    "        # Call the graph\n",
    "        final_output = None\n",
    "        for output in loaded_model.stream({'messages': message_objects}):\n",
    "            final_output = output\n",
    "\n",
    "        if final_output and \"enhancer_node\" in final_output and \"messages\" in final_output[\"enhancer_node\"]:\n",
    "            messages = final_output[\"enhancer_node\"][\"messages\"]\n",
    "            for msg in messages:\n",
    "                if isinstance(msg, AIMessage):\n",
    "                    return msg.content\n",
    "        return \"No response from the chatbot.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7759cccb-ea49-4ae3-ade1-00d36edac822",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "chatbot = LanggraphRunnable(user_name=\"Bob\")\n",
    "\n",
    "history = [\n",
    "    {\"role\": \"user\", \"content\": \"Hi I am Bob, please mention my name before answering all questions. What is the official work-from-home policy?\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"Hi Bob, Employees may work from home up to 3 days per week with manager approval.\"}\n",
    "]\n",
    "\n",
    "user_input = \"Can employees also work from home?\"\n",
    "response = chatbot.invoke(user_input, history=history)\n",
    "print(\"ðŸ¤– Chatbot:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf6d5809-0689-42b5-b845-df84a48ea95a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "user_input = \"How do I do expenses when I travel?\"\n",
    "response = chatbot.invoke(user_input, history=history)\n",
    "print(\"ðŸ¤– Chatbot:\", response)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [
    {
     "elements": [],
     "globalVars": {},
     "guid": "",
     "layoutOption": {
      "grid": true,
      "stack": true
     },
     "nuid": "4c9eb761-8224-4b71-a0f3-534b07f57c5b",
     "origId": 8645040152748842,
     "title": "Untitled",
     "version": "DashboardViewV1",
     "width": 1024
    }
   ],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "4_HR_Chatbot_Deploy_Agent_Based_JIRA_1619",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
